{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3644db",
   "metadata": {},
   "source": [
    "### Scikit Learn GridSearchCV   \n",
    "\n",
    "Johnathan Clementi   \n",
    "25 March, 2022   \n",
    "University of Pennsylvania, Master of Urban Spatial Analytics   \n",
    "Remote Sensing - MUSA 650 - Spring 2022   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88bf57f",
   "metadata": {},
   "source": [
    "The hyper-parameters of a machine learning model are user specified variables that can be adjusted in order to improve the performance of the model. The process of choosing optimal hyper-parameters involves comparing the decision metrics of models with a different hyper-parameter (or combination of hyper-parameters). [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) is a form of Scikit-Learn function which iterates through an exhaustive grid of hyper-parameter combinations, trains a model using those parameters, and then selects the best hyper-parameter combination based on a scoring function.\n",
    "\n",
    "The function signature of the GridSearchCV is: `GridSearchCV(estimator, param_grid, scoring, refit, cv)`\n",
    "\n",
    "The first argument, `estimator`, is a regressor or classifier estimator object such as sklearn.svm.SVC())\n",
    "\n",
    "The `param_grid` is the hyper-parameter space in the form of a Python dictionary. A basic param_grid for an SVC estimator looks like this:    \n",
    "  `param_grid =  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}`   \n",
    "A grid search run with this parameter grid will test 8 models (4 $C$ parameters * 2 gamma parameters * 1 kernel parameter)\n",
    "<table>\n",
    "\t\t<thead>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<th>Model #<br></th>\n",
    "\t\t\t\t<th>C</th>\n",
    "\t\t\t\t<th>gamma</th>\n",
    "\t\t\t\t<th>kernel</th>\n",
    "\t\t\t</tr>\n",
    "\t\t</thead>\n",
    "\t\t<tbody>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>0<br></td>\n",
    "\t\t\t\t<td>1<br></td>\n",
    "\t\t\t\t<td>0.001</td>\n",
    "\t\t\t\t<td>rbf</td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>1</td>\n",
    "\t\t\t\t<td>1</td>\n",
    "\t\t\t\t<td>0.0001</td>\n",
    "\t\t\t\t<td>rbf <br></td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>2</td>\n",
    "\t\t\t\t<td>10</td>\n",
    "\t\t\t\t<td>0.001 <br></td>\n",
    "\t\t\t\t<td>rbf</td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>3</td>\n",
    "\t\t\t\t<td>10</td>\n",
    "\t\t\t\t<td>0.0001 <br></td>\n",
    "\t\t\t\t<td>rbf</td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>4</td>\n",
    "\t\t\t\t<td>100</td>\n",
    "\t\t\t\t<td>0.001</td>\n",
    "\t\t\t\t<td>rbf</td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>5</td>\n",
    "\t\t\t\t<td>100</td>\n",
    "\t\t\t\t<td>0.0001</td>\n",
    "\t\t\t\t<td>rbf</td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>6</td>\n",
    "\t\t\t\t<td>1000</td>\n",
    "\t\t\t\t<td>0.001</td>\n",
    "\t\t\t\t<td>rbf</td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>7</td>\n",
    "\t\t\t\t<td>1000</td>\n",
    "\t\t\t\t<td>0.0001</td>\n",
    "\t\t\t\t<td>rbf</td>\n",
    "\t\t\t</tr>\n",
    "\t\t</tbody>\n",
    "\t</table>\n",
    "\n",
    "\n",
    "The `scoring` argument provides the metric(s) by which the model will be compared. It can be omitted, provided a single string value, or a list/tuple of unique strings for [model scoring metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring) This possible scoring metrics are dictated by the model used (e.g. r2 is only available to regressor estimators)    \n",
    "\n",
    "The `refit` argument dictates whether a new estimator will be trained after the optimal hyper-parameters have been found. The default value is `True`. If `True`, the new refit estimator can be used to predict on outside data [**1**].    \n",
    "\n",
    "The `cv` argument provides the number of folds to run. The default value is 5 folds.   \n",
    "   \n",
    "\n",
    "[**1**] While it is not the purpose of this document to compare non-nested and nested cross-validation, it worth noting that the most robust way to tune hyper-parameters is through nested cross-valiation as there is not a chance of information leakage from model training to testing. For more information, please see Scikit-Learn's article on [Nested versus non-nested cross-validation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html).\n",
    "\n",
    "\n",
    "\n",
    "In this example, we will illustrate the use of the GridSearchCV to tune the hyper-parameters for a model classifying the number in an image in the MNIST digits dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3ae6a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "# Pandas is commonly aliased as pd\n",
    "import pandas as pd\n",
    "# Numpy is commonly aliased as np\n",
    "import numpy as np\n",
    "\n",
    "# The Modified National Institute of Standards and Technology (MNIST) database provides\n",
    "# robust sample datasets which can be accessed through the Keras/Tensorflow libraries.\n",
    "# In this example, we will be using the MNIST hand-written digits dataset\n",
    "# For more information on how to access MNIST data, see https://www.tensorflow.org/datasets/catalog/mnist\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a random state for reproducability:\n",
    "random_state = 321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5998d1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 28, 28)\n",
      "Y_train: (60000,)\n",
      "X_test:  (10000, 28, 28)\n",
      "Y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train: ' + str(X_train.shape))\n",
    "print('Y_train: ' + str(y_train.shape))\n",
    "print('X_test:  '  + str(X_test.shape))\n",
    "print('Y_test:  '  + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4bb7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "for i in range(10):  \n",
    "  plt.subplot(2,5, i+1)\n",
    "  plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efd4e837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize data to pass it to a model\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e672e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "data = datasets.load_digits()\n",
    "X = data.images\n",
    "y = data.target\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "for i in range(10):  \n",
    "  plt.subplot(2,5, i+1)\n",
    "  plt.imshow(X[i], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac45c02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1257, 64)\n",
      "Y_train: (1257,)\n",
      "X_test:  (540, 64)\n",
      "Y_test:  (540,)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize data to pass it to a model\n",
    "X = X.reshape(1797, 64)\n",
    "X = X.astype('float32')\n",
    "X /= 255\n",
    "\n",
    "# Conduct the Train/Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = random_state, stratify = y)\n",
    "\n",
    "print('X_train: ' + str(X_train.shape))\n",
    "print('Y_train: ' + str(y_train.shape))\n",
    "print('X_test:  '  + str(X_test.shape))\n",
    "print('Y_test:  '  + str(y_test.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
